{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost for MNIST Digit Recognition\n",
        "\n",
        "This notebook implements a comprehensive XGBoost classifier for MNIST digit recognition, including:\n",
        "- Data loading and exploration\n",
        "- Preprocessing pipeline\n",
        "- Baseline models (Logistic Regression, Decision Tree)\n",
        "- XGBoost hyperparameter tuning (two-stage)\n",
        "- Other ensemble models (AdaBoost, Gradient Boosting)\n",
        "- Comprehensive evaluation and visualizations\n",
        "- Error analysis and model comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV, learning_curve\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
        "                             precision_recall_fscore_support, roc_auc_score, roc_curve)\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RND = 42\n",
        "np.random.seed(RND)\n",
        "\n",
        "# Set style for plots\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except:\n",
        "        plt.style.use('ggplot')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Inspect Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training set shape: (60000, 784)\n",
            "Test set shape: (10000, 784)\n",
            "\n",
            "Training set class distribution:\n",
            "0    5923\n",
            "1    6742\n",
            "2    5958\n",
            "3    6131\n",
            "4    5842\n",
            "5    5421\n",
            "6    5918\n",
            "7    6265\n",
            "8    5851\n",
            "9    5949\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test set class distribution:\n",
            "0     980\n",
            "1    1135\n",
            "2    1032\n",
            "3    1010\n",
            "4     982\n",
            "5     892\n",
            "6     958\n",
            "7    1028\n",
            "8     974\n",
            "9    1009\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Missing values in training set: 0\n",
            "Missing values in test set: 0\n",
            "\n",
            "Training pixel value range: [0, 255]\n",
            "Test pixel value range: [0, 255]\n"
          ]
        }
      ],
      "source": [
        "# Load training and test data\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv('mnist_train.csv')\n",
        "test_df = pd.read_csv('mnist_test.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = train_df.iloc[:, 1:].values  # All columns except first\n",
        "y_train = train_df.iloc[:, 0].values   # First column (label)\n",
        "X_test = test_df.iloc[:, 1:].values\n",
        "y_test = test_df.iloc[:, 0].values\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "print(pd.Series(y_train).value_counts().sort_index())\n",
        "print(f\"\\nTest set class distribution:\")\n",
        "print(pd.Series(y_test).value_counts().sort_index())\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values in training set: {np.isnan(X_train).sum()}\")\n",
        "print(f\"Missing values in test set: {np.isnan(X_test).sum()}\")\n",
        "\n",
        "# Check pixel value range\n",
        "print(f\"\\nTraining pixel value range: [{X_train.min()}, {X_train.max()}]\")\n",
        "print(f\"Test pixel value range: [{X_test.min()}, {X_test.max()}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize sample digits\n",
        "def plot_sample_digits(X, y, n_samples=10, title=\"Sample Digits\"):\n",
        "    \"\"\"Plot a grid of sample digits\"\"\"\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
        "    for idx, ax in zip(indices, axes):\n",
        "        digit = X[idx].reshape(28, 28)\n",
        "        ax.imshow(digit, cmap='gray')\n",
        "        ax.set_title(f'Label: {int(y[idx])}', fontsize=12)\n",
        "        ax.axis('off')\n",
        "    \n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_digits.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_sample_digits(X_train, y_train, title=\"Sample Training Digits\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaled training range: [0.000, 1.000]\n",
            "Scaled test range: [0.000, 24.000]\n"
          ]
        }
      ],
      "source": [
        "# Create preprocessing pipeline\n",
        "# Since pixel values are in 0-255 range, we'll scale to [0,1]\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Scaled training range: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\n",
        "print(f\"Scaled test range: [{X_test_scaled.min():.3f}, {X_test_scaled.max():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Baseline Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to perform cross-validation\n",
        "def cross_validate_model(model, X, y, cv=5, scoring=['accuracy', 'f1_macro']):\n",
        "    \"\"\"Perform stratified cross-validation and return mean scores\"\"\"\n",
        "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RND)\n",
        "    scores = {'accuracy': [], 'f1_macro': []}\n",
        "    \n",
        "    for train_idx, val_idx in skf.split(X, y):\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "        \n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        y_pred = model.predict(X_val_fold)\n",
        "        \n",
        "        scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))\n",
        "        scores['f1_macro'].append(\n",
        "            precision_recall_fscore_support(y_val_fold, y_pred, average='macro')[2]\n",
        "        )\n",
        "    \n",
        "    return {\n",
        "        'accuracy_mean': np.mean(scores['accuracy']),\n",
        "        'accuracy_std': np.std(scores['accuracy']),\n",
        "        'f1_macro_mean': np.mean(scores['f1_macro']),\n",
        "        'f1_macro_std': np.std(scores['f1_macro'])\n",
        "    }\n",
        "\n",
        "# Store baseline results\n",
        "baseline_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression baseline...\n"
          ]
        }
      ],
      "source": [
        "# Baseline 1: Logistic Regression\n",
        "print(\"Training Logistic Regression baseline...\")\n",
        "start_time = time.time()\n",
        "\n",
        "lr_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('clf', LogisticRegression(\n",
        "        solver='saga',\n",
        "        multi_class='multinomial',\n",
        "        max_iter=1000,\n",
        "        random_state=RND,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "lr_cv_scores = cross_validate_model(lr_pipeline, X_train, y_train)\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "lr_test_pred = lr_pipeline.predict(X_test)\n",
        "lr_test_acc = accuracy_score(y_test, lr_test_pred)\n",
        "\n",
        "lr_time = time.time() - start_time\n",
        "print(f\"Logistic Regression CV Accuracy: {lr_cv_scores['accuracy_mean']:.4f} ± {lr_cv_scores['accuracy_std']:.4f}\")\n",
        "print(f\"Logistic Regression CV F1-macro: {lr_cv_scores['f1_macro_mean']:.4f} ± {lr_cv_scores['f1_macro_std']:.4f}\")\n",
        "print(f\"Logistic Regression Test Accuracy: {lr_test_acc:.4f}\")\n",
        "print(f\"Training time: {lr_time:.2f} seconds\\n\")\n",
        "\n",
        "baseline_results['Logistic Regression'] = {\n",
        "    'cv_accuracy': lr_cv_scores['accuracy_mean'],\n",
        "    'cv_accuracy_std': lr_cv_scores['accuracy_std'],\n",
        "    'cv_f1_macro': lr_cv_scores['f1_macro_mean'],\n",
        "    'cv_f1_macro_std': lr_cv_scores['f1_macro_std'],\n",
        "    'test_accuracy': lr_test_acc,\n",
        "    'model': lr_pipeline,\n",
        "    'time': lr_time\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline 2: Decision Tree\n",
        "print(\"Training Decision Tree baseline...\")\n",
        "start_time = time.time()\n",
        "\n",
        "dt_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('clf', DecisionTreeClassifier(random_state=RND, max_depth=20))\n",
        "])\n",
        "\n",
        "dt_cv_scores = cross_validate_model(dt_pipeline, X_train, y_train)\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "dt_test_pred = dt_pipeline.predict(X_test)\n",
        "dt_test_acc = accuracy_score(y_test, dt_test_pred)\n",
        "\n",
        "dt_time = time.time() - start_time\n",
        "print(f\"Decision Tree CV Accuracy: {dt_cv_scores['accuracy_mean']:.4f} ± {dt_cv_scores['accuracy_std']:.4f}\")\n",
        "print(f\"Decision Tree CV F1-macro: {dt_cv_scores['f1_macro_mean']:.4f} ± {dt_cv_scores['f1_macro_std']:.4f}\")\n",
        "print(f\"Decision Tree Test Accuracy: {dt_test_acc:.4f}\")\n",
        "print(f\"Training time: {dt_time:.2f} seconds\\n\")\n",
        "\n",
        "baseline_results['Decision Tree'] = {\n",
        "    'cv_accuracy': dt_cv_scores['accuracy_mean'],\n",
        "    'cv_accuracy_std': dt_cv_scores['accuracy_std'],\n",
        "    'cv_f1_macro': dt_cv_scores['f1_macro_mean'],\n",
        "    'cv_f1_macro_std': dt_cv_scores['f1_macro_std'],\n",
        "    'test_accuracy': dt_test_acc,\n",
        "    'model': dt_pipeline,\n",
        "    'time': dt_time\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. XGBoost Model Development\n",
        "\n",
        "### 4.1 Coarse Hyperparameter Search (RandomizedSearchCV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage 1: Coarse search with RandomizedSearchCV\n",
        "print(\"Stage 1: Coarse hyperparameter search with RandomizedSearchCV...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Define parameter grid for coarse search\n",
        "xgb_coarse_params = {\n",
        "    'clf__n_estimators': [100, 200, 500, 1000],\n",
        "    'clf__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
        "    'clf__max_depth': [3, 5, 7, 9, 12],\n",
        "    'clf__subsample': [0.5, 0.7, 0.9, 1.0],\n",
        "    'clf__colsample_bytree': [0.5, 0.7, 0.9, 1.0],\n",
        "    'clf__gamma': [0, 0.1, 0.2, 0.5],\n",
        "    'clf__reg_alpha': [0, 0.01, 0.1, 1],\n",
        "    'clf__reg_lambda': [0, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Create XGBoost pipeline\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('clf', xgb.XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        random_state=RND,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Use a subset for faster coarse search (optional - can use full dataset)\n",
        "# For efficiency, we'll use a sample for coarse search\n",
        "sample_size = min(10000, len(X_train))\n",
        "sample_indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
        "X_train_sample = X_train[sample_indices]\n",
        "y_train_sample = y_train[sample_indices]\n",
        "\n",
        "# Randomized search\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RND)  # Use 3-fold for speed\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb_pipeline,\n",
        "    xgb_coarse_params,\n",
        "    n_iter=50,  # Limit iterations for efficiency\n",
        "    cv=skf,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=RND,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "coarse_time = time.time() - start_time\n",
        "print(f\"\\nBest parameters from coarse search:\")\n",
        "print(random_search.best_params_)\n",
        "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
        "print(f\"Coarse search time: {coarse_time:.2f} seconds\")\n",
        "\n",
        "best_coarse_params = random_search.best_params_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage 2: Refined search around best parameters\n",
        "print(\"\\nStage 2: Refined hyperparameter search with GridSearchCV...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Extract best values and create refined grid\n",
        "best_n_est = best_coarse_params['clf__n_estimators']\n",
        "best_lr = best_coarse_params['clf__learning_rate']\n",
        "best_depth = best_coarse_params['clf__max_depth']\n",
        "best_subsample = best_coarse_params['clf__subsample']\n",
        "best_colsample = best_coarse_params['clf__colsample_bytree']\n",
        "best_gamma = best_coarse_params['clf__gamma']\n",
        "best_alpha = best_coarse_params['clf__reg_alpha']\n",
        "best_lambda = best_coarse_params['clf__reg_lambda']\n",
        "\n",
        "# Create refined parameter grid around best values\n",
        "xgb_refined_params = {\n",
        "    'clf__n_estimators': [max(100, best_n_est - 100), best_n_est, min(1000, best_n_est + 100)],\n",
        "    'clf__learning_rate': [\n",
        "        max(0.01, best_lr - 0.02), \n",
        "        best_lr, \n",
        "        min(0.2, best_lr + 0.02)\n",
        "    ],\n",
        "    'clf__max_depth': [\n",
        "        max(3, best_depth - 2), \n",
        "        best_depth, \n",
        "        min(12, best_depth + 2)\n",
        "    ],\n",
        "    'clf__subsample': [\n",
        "        max(0.5, best_subsample - 0.1), \n",
        "        best_subsample, \n",
        "        min(1.0, best_subsample + 0.1)\n",
        "    ],\n",
        "    'clf__colsample_bytree': [\n",
        "        max(0.5, best_colsample - 0.1), \n",
        "        best_colsample, \n",
        "        min(1.0, best_colsample + 0.1)\n",
        "    ],\n",
        "    'clf__gamma': [max(0, best_gamma - 0.1), best_gamma, min(0.5, best_gamma + 0.1)],\n",
        "    'clf__reg_alpha': [max(0, best_alpha - 0.05), best_alpha, min(1, best_alpha + 0.05)],\n",
        "    'clf__reg_lambda': [max(0, best_lambda - 0.05), best_lambda, min(1, best_lambda + 0.05)]\n",
        "}\n",
        "\n",
        "# Remove duplicates and ensure valid ranges\n",
        "for key, values in xgb_refined_params.items():\n",
        "    xgb_refined_params[key] = sorted(list(set([v for v in values if v >= 0])))\n",
        "\n",
        "# Grid search with smaller grid\n",
        "grid_search = GridSearchCV(\n",
        "    xgb_pipeline,\n",
        "    xgb_refined_params,\n",
        "    cv=skf,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "refined_time = time.time() - start_time\n",
        "print(f\"\\nBest parameters from refined search:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Refined search time: {refined_time:.2f} seconds\")\n",
        "\n",
        "best_xgb_params = grid_search.best_params_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final XGBoost model on full training set with best parameters\n",
        "print(\"\\nTraining final XGBoost model on full training set...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create final model with best parameters\n",
        "final_xgb_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('clf', xgb.XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        random_state=RND,\n",
        "        n_jobs=-1,\n",
        "        **{k.replace('clf__', ''): v for k, v in best_xgb_params.items()}\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Cross-validate on full training set\n",
        "xgb_cv_scores = cross_validate_model(final_xgb_pipeline, X_train, y_train, cv=5)\n",
        "\n",
        "# Train on full training set\n",
        "final_xgb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "xgb_test_pred = final_xgb_pipeline.predict(X_test)\n",
        "xgb_test_proba = final_xgb_pipeline.predict_proba(X_test)\n",
        "xgb_test_acc = accuracy_score(y_test, xgb_test_pred)\n",
        "\n",
        "xgb_time = time.time() - start_time\n",
        "print(f\"XGBoost CV Accuracy: {xgb_cv_scores['accuracy_mean']:.4f} ± {xgb_cv_scores['accuracy_std']:.4f}\")\n",
        "print(f\"XGBoost CV F1-macro: {xgb_cv_scores['f1_macro_mean']:.4f} ± {xgb_cv_scores['f1_macro_std']:.4f}\")\n",
        "print(f\"XGBoost Test Accuracy: {xgb_test_acc:.4f}\")\n",
        "print(f\"Training time: {xgb_time:.2f} seconds\")\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(final_xgb_pipeline, 'model_3.pkl')\n",
        "print(\"\\nModel saved as 'model_3.pkl'\")\n",
        "\n",
        "# Store results\n",
        "xgb_results = {\n",
        "    'cv_accuracy': xgb_cv_scores['accuracy_mean'],\n",
        "    'cv_accuracy_std': xgb_cv_scores['accuracy_std'],\n",
        "    'cv_f1_macro': xgb_cv_scores['f1_macro_mean'],\n",
        "    'cv_f1_macro_std': xgb_cv_scores['f1_macro_std'],\n",
        "    'test_accuracy': xgb_test_acc,\n",
        "    'model': final_xgb_pipeline,\n",
        "    'time': xgb_time,\n",
        "    'best_params': best_xgb_params\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test & Evaluate XGBoost Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation of XGBoost on test set\n",
        "print(\"XGBoost Test Set Evaluation:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, xgb_test_pred))\n",
        "\n",
        "# Precision, Recall, F1 (macro, micro, weighted)\n",
        "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    y_test, xgb_test_pred, average='macro'\n",
        ")\n",
        "precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
        "    y_test, xgb_test_pred, average='micro'\n",
        ")\n",
        "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "    y_test, xgb_test_pred, average='weighted'\n",
        ")\n",
        "\n",
        "print(f\"\\nMacro-averaged metrics:\")\n",
        "print(f\"  Precision: {precision_macro:.4f}\")\n",
        "print(f\"  Recall: {recall_macro:.4f}\")\n",
        "print(f\"  F1-score: {f1_macro:.4f}\")\n",
        "\n",
        "print(f\"\\nMicro-averaged metrics:\")\n",
        "print(f\"  Precision: {precision_micro:.4f}\")\n",
        "print(f\"  Recall: {recall_micro:.4f}\")\n",
        "print(f\"  F1-score: {f1_micro:.4f}\")\n",
        "\n",
        "print(f\"\\nWeighted-averaged metrics:\")\n",
        "print(f\"  Precision: {precision_weighted:.4f}\")\n",
        "print(f\"  Recall: {recall_weighted:.4f}\")\n",
        "print(f\"  F1-score: {f1_weighted:.4f}\")\n",
        "\n",
        "# ROC-AUC (One-vs-Rest)\n",
        "try:\n",
        "    roc_auc_macro = roc_auc_score(y_test, xgb_test_proba, average='macro', multi_class='ovr')\n",
        "    roc_auc_micro = roc_auc_score(y_test, xgb_test_proba, average='micro', multi_class='ovr')\n",
        "    print(f\"\\nROC-AUC (One-vs-Rest):\")\n",
        "    print(f\"  Macro: {roc_auc_macro:.4f}\")\n",
        "    print(f\"  Micro: {roc_auc_micro:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nROC-AUC calculation skipped: {e}\")\n",
        "\n",
        "# Store detailed metrics\n",
        "xgb_results['test_precision_macro'] = precision_macro\n",
        "xgb_results['test_recall_macro'] = recall_macro\n",
        "xgb_results['test_f1_macro'] = f1_macro\n",
        "xgb_results['test_precision_micro'] = precision_micro\n",
        "xgb_results['test_recall_micro'] = recall_micro\n",
        "xgb_results['test_f1_micro'] = f1_micro\n",
        "try:\n",
        "    xgb_results['test_roc_auc_macro'] = roc_auc_macro\n",
        "    xgb_results['test_roc_auc_micro'] = roc_auc_micro\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Other Ensemble Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results for all models\n",
        "all_model_results = {\n",
        "    'Logistic Regression': baseline_results['Logistic Regression'],\n",
        "    'Decision Tree': baseline_results['Decision Tree'],\n",
        "    'XGBoost': xgb_results\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AdaBoost Classifier\n",
        "print(\"Training AdaBoost Classifier...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Use estimator parameter (base_estimator is deprecated in newer sklearn versions)\n",
        "ada_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('clf', AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=3, random_state=RND),\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        random_state=RND\n",
        "    ))\n",
        "])\n",
        "\n",
        "ada_cv_scores = cross_validate_model(ada_pipeline, X_train, y_train)\n",
        "ada_pipeline.fit(X_train, y_train)\n",
        "ada_test_pred = ada_pipeline.predict(X_test)\n",
        "ada_test_proba = ada_pipeline.predict_proba(X_test)\n",
        "ada_test_acc = accuracy_score(y_test, ada_test_pred)\n",
        "\n",
        "ada_time = time.time() - start_time\n",
        "print(f\"AdaBoost CV Accuracy: {ada_cv_scores['accuracy_mean']:.4f} ± {ada_cv_scores['accuracy_std']:.4f}\")\n",
        "print(f\"AdaBoost CV F1-macro: {ada_cv_scores['f1_macro_mean']:.4f} ± {ada_cv_scores['f1_macro_std']:.4f}\")\n",
        "print(f\"AdaBoost Test Accuracy: {ada_test_acc:.4f}\")\n",
        "print(f\"Training time: {ada_time:.2f} seconds\\n\")\n",
        "\n",
        "ada_precision_macro, ada_recall_macro, ada_f1_macro, _ = precision_recall_fscore_support(\n",
        "    y_test, ada_test_pred, average='macro'\n",
        ")\n",
        "try:\n",
        "    ada_roc_auc_macro = roc_auc_score(y_test, ada_test_proba, average='macro', multi_class='ovr')\n",
        "    ada_roc_auc_micro = roc_auc_score(y_test, ada_test_proba, average='micro', multi_class='ovr')\n",
        "except:\n",
        "    ada_roc_auc_macro = None\n",
        "    ada_roc_auc_micro = None\n",
        "\n",
        "all_model_results['AdaBoost'] = {\n",
        "    'cv_accuracy': ada_cv_scores['accuracy_mean'],\n",
        "    'cv_accuracy_std': ada_cv_scores['accuracy_std'],\n",
        "    'cv_f1_macro': ada_cv_scores['f1_macro_mean'],\n",
        "    'cv_f1_macro_std': ada_cv_scores['f1_macro_std'],\n",
        "    'test_accuracy': ada_test_acc,\n",
        "    'test_f1_macro': ada_f1_macro,\n",
        "    'test_roc_auc_macro': ada_roc_auc_macro,\n",
        "    'test_roc_auc_micro': ada_roc_auc_micro,\n",
        "    'model': ada_pipeline,\n",
        "    'time': ada_time\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient Boosting Classifier\n",
        "print(\"Training Gradient Boosting Classifier...\")\n",
        "start_time = time.time()\n",
        "\n",
        "gbm_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('clf', GradientBoostingClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        random_state=RND\n",
        "    ))\n",
        "])\n",
        "\n",
        "gbm_cv_scores = cross_validate_model(gbm_pipeline, X_train, y_train)\n",
        "gbm_pipeline.fit(X_train, y_train)\n",
        "gbm_test_pred = gbm_pipeline.predict(X_test)\n",
        "gbm_test_proba = gbm_pipeline.predict_proba(X_test)\n",
        "gbm_test_acc = accuracy_score(y_test, gbm_test_pred)\n",
        "\n",
        "gbm_time = time.time() - start_time\n",
        "print(f\"Gradient Boosting CV Accuracy: {gbm_cv_scores['accuracy_mean']:.4f} ± {gbm_cv_scores['accuracy_std']:.4f}\")\n",
        "print(f\"Gradient Boosting CV F1-macro: {gbm_cv_scores['f1_macro_mean']:.4f} ± {gbm_cv_scores['f1_macro_std']:.4f}\")\n",
        "print(f\"Gradient Boosting Test Accuracy: {gbm_test_acc:.4f}\")\n",
        "print(f\"Training time: {gbm_time:.2f} seconds\\n\")\n",
        "\n",
        "gbm_precision_macro, gbm_recall_macro, gbm_f1_macro, _ = precision_recall_fscore_support(\n",
        "    y_test, gbm_test_pred, average='macro'\n",
        ")\n",
        "try:\n",
        "    gbm_roc_auc_macro = roc_auc_score(y_test, gbm_test_proba, average='macro', multi_class='ovr')\n",
        "    gbm_roc_auc_micro = roc_auc_score(y_test, gbm_test_proba, average='micro', multi_class='ovr')\n",
        "except:\n",
        "    gbm_roc_auc_macro = None\n",
        "    gbm_roc_auc_micro = None\n",
        "\n",
        "all_model_results['Gradient Boosting'] = {\n",
        "    'cv_accuracy': gbm_cv_scores['accuracy_mean'],\n",
        "    'cv_accuracy_std': gbm_cv_scores['accuracy_std'],\n",
        "    'cv_f1_macro': gbm_cv_scores['f1_macro_mean'],\n",
        "    'cv_f1_macro_std': gbm_cv_scores['f1_macro_std'],\n",
        "    'test_accuracy': gbm_test_acc,\n",
        "    'test_f1_macro': gbm_f1_macro,\n",
        "    'test_roc_auc_macro': gbm_roc_auc_macro,\n",
        "    'test_roc_auc_micro': gbm_roc_auc_micro,\n",
        "    'model': gbm_pipeline,\n",
        "    'time': gbm_time\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.1 Confusion Matrix Heatmap for XGBoost\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name=\"XGBoost\", save_name=\"confusion_matrix\"):\n",
        "    \"\"\"Plot annotated confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Raw confusion matrix\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "                xticklabels=range(10), yticklabels=range(10))\n",
        "    axes[0].set_title(f'{model_name} - Confusion Matrix (Raw Counts)', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
        "    axes[0].set_ylabel('True Label', fontsize=12)\n",
        "    \n",
        "    # Normalized confusion matrix\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=axes[1],\n",
        "                xticklabels=range(10), yticklabels=range(10))\n",
        "    axes[1].set_title(f'{model_name} - Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
        "    axes[1].set_ylabel('True Label', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_name}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(y_test, xgb_test_pred, save_name='xgb_confusion_matrix')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.2 Per-class Precision/Recall/F1 Bar Plot\n",
        "def plot_per_class_metrics(y_true, y_pred, model_name=\"XGBoost\", save_name=\"per_class_metrics\"):\n",
        "    \"\"\"Plot per-class precision, recall, and F1 scores\"\"\"\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "    \n",
        "    x = np.arange(10)\n",
        "    width = 0.25\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.bar(x - width, precision, width, label='Precision', alpha=0.8)\n",
        "    ax.bar(x, recall, width, label='Recall', alpha=0.8)\n",
        "    ax.bar(x + width, f1, width, label='F1-Score', alpha=0.8)\n",
        "    \n",
        "    ax.set_xlabel('Digit Class', fontsize=12)\n",
        "    ax.set_ylabel('Score', fontsize=12)\n",
        "    ax.set_title(f'{model_name} - Per-Class Metrics', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(range(10))\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim([0, 1.1])\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i in range(10):\n",
        "        ax.text(i - width, precision[i] + 0.02, f'{precision[i]:.2f}', \n",
        "                ha='center', va='bottom', fontsize=8)\n",
        "        ax.text(i, recall[i] + 0.02, f'{recall[i]:.2f}', \n",
        "                ha='center', va='bottom', fontsize=8)\n",
        "        ax.text(i + width, f1[i] + 0.02, f'{f1[i]:.2f}', \n",
        "                ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_name}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_per_class_metrics(y_test, xgb_test_pred, save_name='xgb_per_class_metrics')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.3 ROC Curves (One-vs-Rest) for XGBoost\n",
        "def plot_roc_curves(y_true, y_proba, model_name=\"XGBoost\", save_name=\"roc_curves\"):\n",
        "    \"\"\"Plot ROC curves for One-vs-Rest multiclass classification\"\"\"\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "    \n",
        "    # Binarize the output\n",
        "    y_test_bin = label_binarize(y_true, classes=list(range(10)))\n",
        "    n_classes = y_test_bin.shape[1]\n",
        "    \n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    \n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
        "        roc_auc[i] = roc_auc_score(y_test_bin[:, i], y_proba[:, i])\n",
        "    \n",
        "    # Compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_proba.ravel())\n",
        "    roc_auc[\"micro\"] = roc_auc_score(y_test_bin, y_proba, average=\"micro\")\n",
        "    \n",
        "    # Compute macro-average ROC curve and ROC area\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "    mean_tpr /= n_classes\n",
        "    fpr[\"macro\"] = all_fpr\n",
        "    tpr[\"macro\"] = mean_tpr\n",
        "    roc_auc[\"macro\"] = roc_auc_score(y_test_bin, y_proba, average=\"macro\")\n",
        "    \n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Plot ROC curves for a subset of classes (0, 1, 2, 3, 4) for clarity\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, 10))\n",
        "    for i in range(min(5, n_classes)):\n",
        "        plt.plot(fpr[i], tpr[i], color=colors[i], lw=2,\n",
        "                label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "    \n",
        "    # Plot macro and micro averages\n",
        "    plt.plot(fpr[\"macro\"], tpr[\"macro\"], \n",
        "            label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.2f})',\n",
        "            color='navy', linestyle='--', linewidth=2)\n",
        "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "            label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.2f})',\n",
        "            color='deeppink', linestyle='--', linewidth=2)\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title(f'{model_name} - ROC Curves (One-vs-Rest)', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_name}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curves(y_test, xgb_test_proba, save_name='xgb_roc_curves')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.4 Learning Curve for XGBoost\n",
        "def plot_learning_curve(estimator, X, y, title=\"Learning Curve\", save_name=\"learning_curve\"):\n",
        "    \"\"\"Plot learning curve showing training and validation scores\"\"\"\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        estimator, X, y, cv=5, n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "        scoring='accuracy',\n",
        "        random_state=RND\n",
        "    )\n",
        "    \n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Accuracy')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
        "    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Accuracy')\n",
        "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
        "    \n",
        "    plt.xlabel('Training Set Size', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_name}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_curve(final_xgb_pipeline, X_train, y_train, \n",
        "                   title=\"XGBoost Learning Curve\", save_name='xgb_learning_curve')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.5 Feature Importance Visualization\n",
        "def plot_feature_importance(model, save_name=\"feature_importance\"):\n",
        "    \"\"\"Plot top feature importances and visualize as 28x28 heatmap\"\"\"\n",
        "    # Get feature importances\n",
        "    if hasattr(model, 'named_steps'):\n",
        "        # Pipeline\n",
        "        importances = model.named_steps['clf'].feature_importances_\n",
        "    else:\n",
        "        importances = model.feature_importances_\n",
        "    \n",
        "    # Top 30 features\n",
        "    top_indices = np.argsort(importances)[-30:][::-1]\n",
        "    top_importances = importances[top_indices]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Bar plot of top 30 features\n",
        "    axes[0].barh(range(30), top_importances)\n",
        "    axes[0].set_yticks(range(30))\n",
        "    axes[0].set_yticklabels([f'Pixel {i}' for i in top_indices])\n",
        "    axes[0].set_xlabel('Importance', fontsize=12)\n",
        "    axes[0].set_title('Top 30 Feature Importances', fontsize=14, fontweight='bold')\n",
        "    axes[0].invert_yaxis()\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # 28x28 heatmap\n",
        "    importance_map = importances.reshape(28, 28)\n",
        "    im = axes[1].imshow(importance_map, cmap='hot', interpolation='nearest')\n",
        "    axes[1].set_title('Feature Importance Heatmap (28x28)', fontsize=14, fontweight='bold')\n",
        "    plt.colorbar(im, ax=axes[1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_name}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_feature_importance(final_xgb_pipeline, save_name='xgb_feature_importance')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.6 Model Comparison Plot\n",
        "def plot_model_comparison(results_dict, save_name=\"model_comparison\"):\n",
        "    \"\"\"Plot comparison of all models\"\"\"\n",
        "    models = list(results_dict.keys())\n",
        "    cv_accs = [results_dict[m]['cv_accuracy'] for m in models]\n",
        "    cv_acc_stds = [results_dict[m].get('cv_accuracy_std', 0) for m in models]\n",
        "    test_accs = [results_dict[m]['test_accuracy'] for m in models]\n",
        "    cv_f1s = [results_dict[m]['cv_f1_macro'] for m in models]\n",
        "    cv_f1_stds = [results_dict[m].get('cv_f1_macro_std', 0) for m in models]\n",
        "    test_f1s = [results_dict[m].get('test_f1_macro', 0) for m in models]\n",
        "    \n",
        "    x = np.arange(len(models))\n",
        "    width = 0.35\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Accuracy comparison\n",
        "    ax1.bar(x - width/2, cv_accs, width, yerr=cv_acc_stds, label='CV Accuracy', alpha=0.8, capsize=5)\n",
        "    ax1.bar(x + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)\n",
        "    ax1.set_xlabel('Model', fontsize=12)\n",
        "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax1.set_title('Model Comparison - Accuracy', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax1.legend()\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "    ax1.set_ylim([0, 1.1])\n",
        "    \n",
        "    # F1-score comparison\n",
        "    ax2.bar(x - width/2, cv_f1s, width, yerr=cv_f1_stds, label='CV F1-macro', alpha=0.8, capsize=5)\n",
        "    ax2.bar(x + width/2, test_f1s, width, label='Test F1-macro', alpha=0.8)\n",
        "    ax2.set_xlabel('Model', fontsize=12)\n",
        "    ax2.set_ylabel('F1-Score (Macro)', fontsize=12)\n",
        "    ax2.set_title('Model Comparison - F1-Score', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax2.legend()\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "    ax2.set_ylim([0, 1.1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_name}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_model_comparison(all_model_results, save_name='model_comparison')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Error Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8.1 Find most confused class pairs\n",
        "def find_confused_pairs(y_true, y_pred):\n",
        "    \"\"\"Find the most confused class pairs from confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    confused_pairs = []\n",
        "    \n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                confused_pairs.append((i, j, cm[i, j]))\n",
        "    \n",
        "    confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    return confused_pairs[:10]  # Top 10\n",
        "\n",
        "confused_pairs = find_confused_pairs(y_test, xgb_test_pred)\n",
        "print(\"Most confused class pairs:\")\n",
        "for true_label, pred_label, count in confused_pairs:\n",
        "    print(f\"  True: {true_label}, Predicted: {pred_label}, Count: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8.2 Visualize misclassified examples\n",
        "def plot_misclassified_examples(X, y_true, y_pred, n_examples=10, save_name=\"misclassified\"):\n",
        "    \"\"\"Plot examples of misclassified digits\"\"\"\n",
        "    misclassified_indices = np.where(y_true != y_pred)[0]\n",
        "    \n",
        "    if len(misclassified_indices) == 0:\n",
        "        print(\"No misclassified examples found!\")\n",
        "        return\n",
        "    \n",
        "    n_examples = min(n_examples, len(misclassified_indices))\n",
        "    selected_indices = np.random.choice(misclassified_indices, n_examples, replace=False)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for idx, ax in zip(selected_indices, axes):\n",
        "        digit = X[idx].reshape(28, 28)\n",
        "        ax.imshow(digit, cmap='gray')\n",
        "        ax.set_title(f'True: {int(y_true[idx])}, Pred: {int(y_pred[idx])}', \n",
        "                    fontsize=11, color='red' if y_true[idx] != y_pred[idx] else 'green')\n",
        "        ax.axis('off')\n",
        "    \n",
        "    plt.suptitle('Misclassified Examples', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_name}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_misclassified_examples(X_test, y_test, xgb_test_pred, save_name='xgb_misclassified')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Model Performance Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive summary table\n",
        "summary_data = []\n",
        "for model_name, results in all_model_results.items():\n",
        "    summary_data.append({\n",
        "        'Model': model_name,\n",
        "        'CV Accuracy': f\"{results['cv_accuracy']:.4f} ± {results.get('cv_accuracy_std', 0):.4f}\",\n",
        "        'CV F1-macro': f\"{results['cv_f1_macro']:.4f} ± {results.get('cv_f1_macro_std', 0):.4f}\",\n",
        "        'Test Accuracy': f\"{results['test_accuracy']:.4f}\",\n",
        "        'Test F1-macro': f\"{results.get('test_f1_macro', 0):.4f}\",\n",
        "        'Training Time (s)': f\"{results['time']:.2f}\"\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_df.to_csv('model_performance_summary.csv', index=False)\n",
        "print(\"\\nSummary saved to 'model_performance_summary.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final hyperparameters\n",
        "print(\"\\nFinal XGBoost Hyperparameters:\")\n",
        "print(\"=\"*50)\n",
        "for key, value in best_xgb_params.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Save hyperparameters to file\n",
        "import json\n",
        "hyperparams_dict = {k.replace('clf__', ''): float(v) if isinstance(v, (int, float, np.integer, np.floating)) else v \n",
        "                   for k, v in best_xgb_params.items()}\n",
        "with open('xgb_hyperparameters.json', 'w') as f:\n",
        "    json.dump(hyperparams_dict, f, indent=2)\n",
        "print(\"\\nHyperparameters saved to 'xgb_hyperparameters.json'\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
